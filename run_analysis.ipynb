{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import datediff, to_date, when, lit, col\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the PySpark context\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"PlanningApp\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = sqlContext.read.json('planning-applications-weekly-list.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for writing to CSV file (escaping strings where necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv(file_name, rows, column_names, row_format_fun):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        csv_writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        csv_writer.writerow(column_names)\n",
    "        for row in rows:\n",
    "            csv_writer.writerow(row_format_fun(row))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Discover the schema of the input dataset and output it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema\n",
    "\n",
    "write_csv('schema.csv',\n",
    "          schema,\n",
    "          ('name', 'dataType', 'nullable'),\n",
    "          lambda r: (r.name, r.dataType, r.nullable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What is the total number of planning application records in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of planning application records: 17805\n"
     ]
    }
   ],
   "source": [
    "n_records = str(df.count())\n",
    "print(\"Total number of planning application records: {0}\".format(n_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Identify the set of case officers (CASEOFFICER field) and output a unique list of these to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column = 'CASEOFFICER'\n",
    "case_officers = (df.select(column).\n",
    "                 distinct().\n",
    "                 sort(column).\n",
    "                 rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "write_csv('case_officers.csv',\n",
    "          case_officers,\n",
    "          (column,),\n",
    "          lambda r: (r,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4) Who are the top N agents (AGENT field) submitting the most number of applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define helper function for replacing empty strings with Null entries.\n",
    "# We do this because it doesn't appear appropriate to include 'missing data' entries for this query\n",
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_agents = 10\n",
    "column = 'AGENT'\n",
    "agent_application_counts = (df.select(column).\n",
    "                            withColumn(column, replace(col(column), '')).\n",
    "                            groupBy(column).\n",
    "                            agg({column: 'count'}).\n",
    "                            orderBy('count(' + column + ')', ascending=False).\n",
    "                            limit(n_top_agents).\n",
    "                            rdd.map(lambda r: (r[0], r[1])).\n",
    "                            collect())\n",
    "\n",
    "write_csv('top_agents_by_application_counts.csv',\n",
    "          agent_application_counts,\n",
    "          ('AGENT','APPLICATION COUNT'),\n",
    "          lambda r: r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5) Count the occurrence of each word within the case text (CASETEXT field) across all planning application records. Output each word and the corresponding count to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_nonalphabetic = re.compile(r\"[^a-z]\")\n",
    "stopword_filter = lambda s: len(s) > 3\n",
    "\n",
    "counts = (df.select('CASETEXT').\n",
    "          rdd.map(lambda row: row[0]).\n",
    "          # Obtain collection of lower-case tokens by splitting at whitespace          \n",
    "          flatMap(lambda s: s.lower().split()).\n",
    "          # Segmentation heuristic which permits punctuated word boundaries without any whitespace\n",
    "          # (w1,w2 w1/w2 w1&w2 etc.)\n",
    "          # Also ensures that non-alphabetic words are filtered out.\n",
    "          flatMap(lambda s: regex_nonalphabetic.split(s)).\n",
    "          # Basic stopword filtering heuristic\n",
    "          filter(stopword_filter).countByValue())\n",
    "\n",
    "counts = sorted(sorted(counts.items()),\n",
    "                key=lambda item: item[1],\n",
    "                reverse=True)\n",
    "\n",
    "write_csv('word_counts.csv',\n",
    "          counts,\n",
    "          ('WORD', 'COUNT'),\n",
    "          lambda r: r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Measure the average public consultation duration in days (i.e. the difference between PUBLICCONSULTATIONENDDATE and PUBLICCONSULTATIONSTARTDATE fields):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average consultation duration in days: 7.1\n"
     ]
    }
   ],
   "source": [
    "mean_diff = (df.withColumn(\"DIFF\", datediff(\n",
    "    to_date('PUBLICCONSULTATIONSTARTDATE', 'dd/MM/YYYY'),\n",
    "    to_date('PUBLICCONSULTATIONENDDATE', 'dd/MM/YYYY'))).\n",
    "             selectExpr('MEAN(DIFF)').\n",
    "             rdd.map(lambda r: r[0]).\n",
    "             collect()[0])\n",
    "\n",
    "print(\"Average consultation duration in days: {:.1f}\".format(mean_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the PySpark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
